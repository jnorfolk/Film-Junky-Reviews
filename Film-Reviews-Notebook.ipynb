{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Film Junky Union Reviews - with BERT and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Film Junky Union, a new edgy community for classic movie enthusiasts, is developing a system for filtering and categorizing movie reviews. The goal is to train a model to automatically detect negative reviews. You'll be using a dataset of IMBD movie reviews with polarity labelling to build a model for classifying positive and negative reviews. It will need to have an F1 score of at least 0.85."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "# the next line provides graphs of better quality on HiDPI screens\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to use progress_apply, read more at https://pypi.org/project/tqdm/#pandas-integration\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_reviews = pd.read_csv('/datasets/imdb_reviews.tsv', sep='\\t', dtype={'votes': 'Int64'})\n",
    "except:\n",
    "    df_reviews = pd.read_csv('imdb_reviews.tsv', sep='\\t', dtype={'votes': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two missing values in a couple columns, but the review column is intact, so I will leave those be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end_year column doesn't seem to add any value to the first few observations. Same goes for the title columns, but we will take a look at the distributions for the columns next. Regardless, the focus in this dataset is on reviews and pos. I want to see if there are any duplicate reviews, and whether or not there is a logical reason for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dup = df_reviews.review.duplicated()\n",
    "dup_idx = np.where(dup)[0]\n",
    "print(df_reviews.loc[dup_idx, 'pos'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be about 100 duplicate reviews. They do not follow a trend in terms of whether they are all positive or all negative, so I will just remove these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.drop_duplicates(subset='review', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.pos.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two values are limited to 0 and 1 for pos which is correct. There are slightly fewer positive reviews than negative reviews, but the difference is negligible. Class balance looks good overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of movies and reviews over years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "ax = axs[0]\n",
    "\n",
    "dft1 = df_reviews[['tconst', 'start_year']].drop_duplicates() \\\n",
    "    ['start_year'].value_counts().sort_index()\n",
    "dft1 = dft1.reindex(index=np.arange(dft1.index.min(), max(dft1.index.max(), 2021))).fillna(0)\n",
    "dft1.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Number of Movies Over Years')\n",
    "\n",
    "ax = axs[1]\n",
    "\n",
    "dft2 = df_reviews.groupby(['start_year', 'pos'])['pos'].count().unstack()\n",
    "dft2 = dft2.reindex(index=np.arange(dft2.index.min(), max(dft2.index.max(), 2021))).fillna(0)\n",
    "\n",
    "dft2.plot(kind='bar', stacked=True, label='#reviews (neg, pos)', ax=ax)\n",
    "\n",
    "dft2 = df_reviews['start_year'].value_counts().sort_index()\n",
    "dft2 = dft2.reindex(index=np.arange(dft2.index.min(), max(dft2.index.max(), 2021))).fillna(0)\n",
    "dft3 = (dft2/dft1).fillna(0)\n",
    "axt = ax.twinx()\n",
    "dft3.reset_index(drop=True).rolling(5).mean().plot(color='orange', label='reviews per movie (avg over 5 years)', ax=axt)\n",
    "\n",
    "lines, labels = axt.get_legend_handles_labels()\n",
    "ax.legend(lines, labels, loc='upper left')\n",
    "\n",
    "ax.set_title('Number of Reviews Over Years')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most movies look to be from the 1990's/2000's, but there are movies from as early as the 1890's. There are a lot fewer movies per year up until around 1970, but these scarcer movies generated almost as many reviews in total as the sum of movies per year did in more present times - these few movies must be classics. The 1960's sees an unexpected drop in movies and movie reviews, but that shouldn't affect our sentiment analysis. It would be curious to determine whether reviews for movies pre-1990 or so were written in those time periods or written in modern times, because the language used may differ and affect the performance of our models. Class balance seems about balanced even looking at each individual year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the distribution of number of reviews per movie with the exact counting and KDE (just to learn how it may differ from the exact counting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "ax = axs[0]\n",
    "dft = df_reviews.groupby('tconst')['review'].count() \\\n",
    "    .value_counts() \\\n",
    "    .sort_index()\n",
    "dft.plot.bar(ax=ax)\n",
    "ax.set_title('Bar Plot of #Reviews Per Movie')\n",
    "\n",
    "ax = axs[1]\n",
    "dft = df_reviews.groupby('tconst')['review'].count()\n",
    "sns.kdeplot(dft, ax=ax)\n",
    "ax.set_title('KDE Plot of #Reviews Per Movie')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most movies only have one or a few reviews apiece. About 400 movies have 30 or more reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews['pos'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axs[0]\n",
    "dft = df_reviews.query('ds_part == \"train\"')['rating'].value_counts().sort_index()\n",
    "dft = dft.reindex(index=np.arange(min(dft.index.min(), 1), max(dft.index.max(), 11))).fillna(0)\n",
    "dft.plot.bar(ax=ax)\n",
    "ax.set_ylim([0, 5000])\n",
    "ax.set_title('The train set: distribution of ratings')\n",
    "\n",
    "ax = axs[1]\n",
    "dft = df_reviews.query('ds_part == \"test\"')['rating'].value_counts().sort_index()\n",
    "dft = dft.reindex(index=np.arange(min(dft.index.min(), 1), max(dft.index.max(), 11))).fillna(0)\n",
    "dft.plot.bar(ax=ax)\n",
    "ax.set_ylim([0, 5000])\n",
    "ax.set_title('The test set: distribution of ratings')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are balanced very well. I'm glad that reviews that gave a movie a rating of 5 or 6 have been omitted, as these are not obviously positive or negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of negative and positive reviews over the years for two parts of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(16, 8), gridspec_kw=dict(width_ratios=(2, 1), height_ratios=(1, 1)))\n",
    "\n",
    "ax = axs[0][0]\n",
    "\n",
    "dft = df_reviews.query('ds_part == \"train\"').groupby(['start_year', 'pos'])['pos'].count().unstack()\n",
    "dft.index = dft.index.astype('int')\n",
    "dft = dft.reindex(index=np.arange(dft.index.min(), max(dft.index.max(), 2020))).fillna(0)\n",
    "dft.plot(kind='bar', stacked=True, ax=ax)\n",
    "ax.set_title('The train set: number of reviews of different polarities per year')\n",
    "\n",
    "ax = axs[0][1]\n",
    "\n",
    "dft = df_reviews.query('ds_part == \"train\"').groupby(['tconst', 'pos'])['pos'].count().unstack()\n",
    "sns.kdeplot(dft[0], color='blue', label='negative', kernel='epa', ax=ax)\n",
    "sns.kdeplot(dft[1], color='green', label='positive', kernel='epa', ax=ax)\n",
    "ax.legend()\n",
    "ax.set_title('The train set: distribution of different polarities per movie')\n",
    "\n",
    "ax = axs[1][0]\n",
    "\n",
    "dft = df_reviews.query('ds_part == \"test\"').groupby(['start_year', 'pos'])['pos'].count().unstack()\n",
    "dft.index = dft.index.astype('int')\n",
    "dft = dft.reindex(index=np.arange(dft.index.min(), max(dft.index.max(), 2020))).fillna(0)\n",
    "dft.plot(kind='bar', stacked=True, ax=ax)\n",
    "ax.set_title('The test set: number of reviews of different polarities per year')\n",
    "\n",
    "ax = axs[1][1]\n",
    "\n",
    "dft = df_reviews.query('ds_part == \"test\"').groupby(['tconst', 'pos'])['pos'].count().unstack()\n",
    "sns.kdeplot(dft[0], color='blue', label='negative', kernel='epa', ax=ax)\n",
    "sns.kdeplot(dft[1], color='green', label='positive', kernel='epa', ax=ax)\n",
    "ax.legend()\n",
    "ax.set_title('The test set: distribution of different polarities per movie')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes look very balanced even when looking at individual years. The class distribution curves look about identical between the training and the test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Composing an evaluation routine which can be used for all models in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def evaluate_model(model, train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    eval_stats = {}\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 6)) \n",
    "    \n",
    "    for type, features, target in (('train', train_features, train_target), ('test', test_features, test_target)):\n",
    "        \n",
    "        eval_stats[type] = {}\n",
    "    \n",
    "        pred_target = model.predict(features)\n",
    "        pred_proba = model.predict_proba(features)[:, 1]\n",
    "        \n",
    "        # F1\n",
    "        f1_thresholds = np.arange(0, 1.01, 0.05)\n",
    "        f1_scores = [metrics.f1_score(target, pred_proba>=threshold) for threshold in f1_thresholds]\n",
    "        \n",
    "        # ROC\n",
    "        fpr, tpr, roc_thresholds = metrics.roc_curve(target, pred_proba)\n",
    "        roc_auc = metrics.roc_auc_score(target, pred_proba)    \n",
    "        eval_stats[type]['ROC AUC'] = roc_auc\n",
    "\n",
    "        # PRC\n",
    "        precision, recall, pr_thresholds = metrics.precision_recall_curve(target, pred_proba)\n",
    "        aps = metrics.average_precision_score(target, pred_proba)\n",
    "        eval_stats[type]['APS'] = aps\n",
    "        \n",
    "        if type == 'train':\n",
    "            color = 'blue'\n",
    "        else:\n",
    "            color = 'green'\n",
    "\n",
    "        # F1 Score\n",
    "        ax = axs[0]\n",
    "        max_f1_score_idx = np.argmax(f1_scores)\n",
    "        ax.plot(f1_thresholds, f1_scores, color=color, label=f'{type}, max={f1_scores[max_f1_score_idx]:.2f} @ {f1_thresholds[max_f1_score_idx]:.2f}')\n",
    "        # setting crosses for some thresholds\n",
    "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
    "            closest_value_idx = np.argmin(np.abs(f1_thresholds-threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'\n",
    "            ax.plot(f1_thresholds[closest_value_idx], f1_scores[closest_value_idx], color=marker_color, marker='X', markersize=7)\n",
    "        ax.set_xlim([-0.02, 1.02])    \n",
    "        ax.set_ylim([-0.02, 1.02])\n",
    "        ax.set_xlabel('threshold')\n",
    "        ax.set_ylabel('F1')\n",
    "        ax.legend(loc='lower center')\n",
    "        ax.set_title(f'F1 Score') \n",
    "\n",
    "        # ROC\n",
    "        ax = axs[1]    \n",
    "        ax.plot(fpr, tpr, color=color, label=f'{type}, ROC AUC={roc_auc:.2f}')\n",
    "        # setting crosses for some thresholds\n",
    "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
    "            closest_value_idx = np.argmin(np.abs(roc_thresholds-threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'            \n",
    "            ax.plot(fpr[closest_value_idx], tpr[closest_value_idx], color=marker_color, marker='X', markersize=7)\n",
    "        ax.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "        ax.set_xlim([-0.02, 1.02])    \n",
    "        ax.set_ylim([-0.02, 1.02])\n",
    "        ax.set_xlabel('FPR')\n",
    "        ax.set_ylabel('TPR')\n",
    "        ax.legend(loc='lower center')        \n",
    "        ax.set_title(f'ROC Curve')\n",
    "        \n",
    "        # PRC\n",
    "        ax = axs[2]\n",
    "        ax.plot(recall, precision, color=color, label=f'{type}, AP={aps:.2f}')\n",
    "        # setting crosses for some thresholds\n",
    "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
    "            closest_value_idx = np.argmin(np.abs(pr_thresholds-threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'\n",
    "            ax.plot(recall[closest_value_idx], precision[closest_value_idx], color=marker_color, marker='X', markersize=7)\n",
    "        ax.set_xlim([-0.02, 1.02])    \n",
    "        ax.set_ylim([-0.02, 1.02])\n",
    "        ax.set_xlabel('recall')\n",
    "        ax.set_ylabel('precision')\n",
    "        ax.legend(loc='lower center')\n",
    "        ax.set_title(f'PRC')        \n",
    "\n",
    "        eval_stats[type]['Accuracy'] = metrics.accuracy_score(target, pred_target)\n",
    "        eval_stats[type]['F1'] = metrics.f1_score(target, pred_target)\n",
    "    \n",
    "    df_eval_stats = pd.DataFrame(eval_stats)\n",
    "    df_eval_stats = df_eval_stats.round(2)\n",
    "    df_eval_stats = df_eval_stats.reindex(index=('Accuracy', 'F1', 'APS', 'ROC AUC'))\n",
    "    \n",
    "    print(df_eval_stats)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume all models below accepts texts in lowercase and without any digits, punctuations marks etc. To accomplish this, we will make everything lowercase and remove anything that isn't a letter. I will also employ stopword removal and lemmatization to clean up the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "corpus = df_reviews.review\n",
    "\n",
    "def clear_text(text):\n",
    "    \n",
    "    pattern = r\"[^a-zA-Z ]\"\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    text = text.split()\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "def normalize(text):\n",
    "\n",
    "    cleared_text = clear_text(text)    \n",
    "    lowered = cleared_text.lower()\n",
    "    tokenized = word_tokenize(lowered)\n",
    "    text_no_stops = [word for word in tokenized if word not in stop_words]\n",
    "    text_joined = nlp(' '.join(text_no_stops))\n",
    "    lemmas = [token.lemma_ for token in text_joined]    \n",
    "    return(' '.join(lemmas))\n",
    "\n",
    "df_reviews['review_norm'] = df_reviews.review.apply(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split & More Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, the whole dataset is already divided into train/test one parts. The corresponding flag is 'ds_part'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_train = df_reviews.query('ds_part == \"train\"').copy()\n",
    "df_reviews_test = df_reviews.query('ds_part == \"test\"').copy()\n",
    "\n",
    "target_train = df_reviews_train['pos']\n",
    "target_test = df_reviews_test['pos']\n",
    "\n",
    "print(df_reviews_train.shape)\n",
    "print(df_reviews_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "count_tf_idf = TfidfVectorizer()\n",
    "corpus = df_reviews_train.review\n",
    "count_tf_idf.fit(corpus)\n",
    "features_train = count_tf_idf.transform(corpus)\n",
    "features_test = count_tf_idf.transform(df_reviews_test.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0 - Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = DummyClassifier(strategy='stratified', random_state=0)\n",
    "model_0.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_0, features_train, target_train, features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a stratified dummy model yielded scores of 0.5 across the board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use logistic regression as our first \"real\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = LogisticRegression(solver='liblinear', random_state=0)\n",
    "model_1.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_1, features_train, target_train, features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tested F1 score for this logistic regression model is at 0.88, which is already higher than the 0.85 F1 score requirement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = cb.CatBoostClassifier(verbose=100, random_state=0, n_estimators=1000, learning_rate=0.12, max_depth=5, early_stopping_rounds=20)\n",
    "%time model_2.fit(features_train, target_train, plot=True, eval_set=(features_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_2, features_train, target_train, features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This CatBoost model performs about as well on the test set as the logistic regression model, and takes much longer to train. The model captures the training data excellently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to also use a SVM model, which seems to be a popular type of model to use for text-based classification tasks. I would have liked to focus on two of its hyperparameters: inverse regularization strength and kernel type. However, this seems to take a very long time to train and evaluate, so I will simply use its default settings. I already used logistic regression without tweaking hyperparameters, so I am interested to compare the performance of these two un-tweaked models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model_3 = SVC(random_state=0, probability=True)\n",
    "model_3.fit(features_train, target_train)\n",
    "\n",
    "evaluate_model(model_3, features_train, target_train, features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I expected a better tested F1 score from the SVM model than I got from the logistic regression model, but I suppose this may have been improved by changing the hyperparameters. The training results are impeccable. This model took a very long time to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model 9 - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "config = transformers.BertConfig.from_pretrained('bert-base-uncased')\n",
    "model = transformers.BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BERT_text_to_embeddings(texts, max_length=512, batch_size=100, force_device=None, disable_progress_bar=False):\n",
    "    \n",
    "    ids_list = []\n",
    "    attention_mask_list = []\n",
    "\n",
    "    # text to padded ids of tokens along with their attention masks\n",
    "    \n",
    "    for input_text in texts:\n",
    "        \n",
    "        ids = tokenizer.encode(\n",
    "            input_text.lower(),\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        \n",
    "        padded = np.array(ids + [0] * (max_length - len(ids)))\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "        ids_list.append(padded)\n",
    "        attention_mask_list.append(attention_mask) \n",
    "    \n",
    "    if force_device is not None:\n",
    "        device = torch.device(force_device)\n",
    "    else:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    model.to(device)\n",
    "    if not disable_progress_bar:\n",
    "        print(f'Using the {device} device.')\n",
    "    \n",
    "    # gettings embeddings in batches\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for i in tqdm(range(math.ceil(len(ids_list)/batch_size)), disable=disable_progress_bar):\n",
    "            \n",
    "        ids_batch = torch.LongTensor(ids_list[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "        \n",
    "        attention_mask_batch = torch.LongTensor(attention_mask_list[batch_size * i : batch_size * (i + 1)])\n",
    "            \n",
    "        with torch.no_grad():            \n",
    "            model.eval()\n",
    "            batch_embeddings = model(input_ids=ids_batch, attention_mask=attention_mask_batch)   \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].detach().cpu().numpy())\n",
    "        \n",
    "    return np.concatenate(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention! Running BERT for thousands of texts may take long run on CPU, at least several hours\n",
    "train_features_9 = BERT_text_to_embeddings(df_reviews_train['review'][:3000], \n",
    "#                                           force_device='cuda'\n",
    "                                          )\n",
    "test_features_9 = BERT_text_to_embeddings(df_reviews_test['review'][:1000],\n",
    "                                          # force_device='cuda'\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train_9 = df_reviews_train.pos[:3000]\n",
    "target_test_9 = df_reviews_test.pos[:1000]\n",
    "\n",
    "print(df_reviews_train['review_norm'].shape)\n",
    "print(train_features_9.shape)\n",
    "print(target_train_9.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_9 = LogisticRegression(max_iter=1000, random_state=0)\n",
    "model_9.fit(train_features_9, target_train_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have got the embeddings, it's advisable to save them to have them ready if \n",
    "np.savez_compressed('features_9.npz', train_features_9=train_features_9, test_features_9=test_features_9)\n",
    "\n",
    "# and load...\n",
    "# with np.load('features_9.npz') as data:\n",
    "#     train_features_9 = data['train_features_9']\n",
    "#     test_features_9 = data['test_features_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "pred_9_train = model_9.predict(train_features_9)\n",
    "score_9_train = f1_score(target_train_9, pred_9_train)\n",
    "print('Training F1 score:', score_9_train)\n",
    "\n",
    "pred_9_test = model_9.predict(test_features_9)\n",
    "score_9_test = f1_score(target_test_9, pred_9_test)\n",
    "print('Tested F1 score', score_9_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first got the BERT embeddings using the fully normalized/lemmatized reviews, but with logistic regression I only got an F1 score as high as 0.77. Next I tried getting the embeddings using the raw reviews, which has gotten me this tested F1 score of 0.84. I got an F1 score of 0.88 using lemmatization and other normalization tricks with logistic regression earlier in the project, so that model/preprocessing combination is still my top pick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to completely remove these reviews and try your models on your own reviews, those below are just examples\n",
    "\n",
    "my_reviews = pd.DataFrame([\n",
    "    'I did not simply like it, not my kind of movie.',\n",
    "    'Well, I was bored and felt asleep in the middle of the movie.',\n",
    "    'I was really fascinated with the movie',    \n",
    "    'Even the actors looked really old and disinterested, and they got paid to be in the movie. What a soulless cash grab.',\n",
    "    'I didn\\'t expect the reboot to be so good! Writers really cared about the source material',\n",
    "    'The movie had its upsides and downsides, but I feel like overall it\\'s a decent flick. I could see myself going to see it again.',\n",
    "    'What a rotten attempt at a comedy. Not a single joke lands, everyone acts annoying and loud, even kids won\\'t like this!',\n",
    "    'Launching on Netflix was a brave move & I really appreciate being able to binge on episode after episode, of this exciting intelligent new drama.'\n",
    "    'The movie was really good, but it could have been a little better.',\n",
    "    'I thought the movie was lit.',\n",
    "    'The movie was not bad, not good.',\n",
    "    'Watching the movie felt like watching paint dry.',\n",
    "    \"I'm on the fence about this particular movie. The writing was decent, the acting was terrible. I would not watch it again.\",\n",
    "], columns=['review'])\n",
    "\n",
    "my_reviews['review_norm'] = my_reviews.review.apply(normalize)\n",
    "\n",
    "my_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = my_reviews['review_norm']\n",
    "\n",
    "my_reviews_pred_prob = model_1.predict_proba(count_tf_idf.transform(texts))[:, 1]\n",
    "\n",
    "for i, review in enumerate(texts.str.slice(0, 100)):\n",
    "    print(f'{my_reviews_pred_prob[i]:.2f}:  {review}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = my_reviews['review_norm']\n",
    "\n",
    "my_reviews_pred_prob = model_2.predict_proba(count_tf_idf.transform(texts))[:, 1]\n",
    "\n",
    "for i, review in enumerate(texts.str.slice(0, 100)):\n",
    "    print(f'{my_reviews_pred_prob[i]:.2f}:  {review}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = my_reviews['review_norm']\n",
    "\n",
    "my_reviews_pred_prob = model_3.predict_proba(count_tf_idf.transform(texts))[:, 1]\n",
    "\n",
    "for i, review in enumerate(texts.str.slice(0, 100)):\n",
    "    print(f'{my_reviews_pred_prob[i]:.2f}:  {review}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = my_reviews['review']\n",
    "\n",
    "my_reviews_features_9 = BERT_text_to_embeddings(texts, disable_progress_bar=True)\n",
    "\n",
    "my_reviews_pred_prob = model_9.predict_proba(my_reviews_features_9)[:, 1]\n",
    "\n",
    "for i, review in enumerate(texts.str.slice(0, 100)):\n",
    "    print(f'{my_reviews_pred_prob[i]:.2f}:  {review}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although all four models had F1 scores of 0.84-0.88, which is solid, they definitely provided wrong answers for some responses - especially in the models that relied on lemmatized data and stopword removal. This normalization style removed the context from words which tended to change their meaning. Personally, I would not be able to answer conclusively about the sentiment of some normalized sentences from this batch, so I cannot blame the model for also being confused. BERT/LR seemed to have the best results, and I believe context is a big reason. Compared to the first LR model, this LR model is more confident on the more clear-cut cases, and shows a little less confidence on less clear-cut cases, but still gets almost all reviews correct. I am surprised that BERT did not have a better tested F1 score, but perhaps training on the full set or using a different model would elevate the results. Training models, tuning hyperparameters, and running preprocessing/embedding takes so long that I am happy enough with my results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started off with a pretty clean dataset, and we found that the classes were already balanced almost perfectly. I normalized the raw reviews by removing non-letter characters, lowercasing everything, removing stop-words, and lemmatizing the text. From this normalized text, I created features by using TF-IDF, and trained logistic regression, CatBoost, and support vector machines with these features. The requirement for F1 score was 0.85, and all three of these models exceeded that. Logistic regression and SVM had the best F1 scores with 0.88 (though CatBoost was close with 0.87), but logistic regression trained much faster than SVM. It was difficult to tune hyperparameters for CatBoost and SVM due to the amount of time required for training. SVM took over an hour with default settings, so I didn't adjust anything with that model, though I suspect performance could have improved with some tuning. I also trained a logistic regression model with BERT embeddings, though I only used a training set of a few thousand reviews. The F1 score of 0.84 that the LR model yielded would likely have been improved by training on the entire dataset rather than a small fraction. Examining each model's decisions on a few short, specific reviews revealed the strength of using BERT embeddings. \n",
    "\n",
    "Logistic regression with lemmatized/TF-IDF features is my choice for top model based on its tested F1 score of 0.88 and its training speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Notebook was opened\n",
    "- [ ]  The text data is loaded and pre-processed for vectorization\n",
    "- [ ]  The text data is transformed to vectors\n",
    "- [ ]  Models are trained and tested\n",
    "- [ ]  The metric's threshold is reached\n",
    "- [ ]  All the code cells are arranged in the order of their execution\n",
    "- [ ]  All the code cells can be executed without errors\n",
    "- [ ]  There are conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
